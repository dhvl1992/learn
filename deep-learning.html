<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Learning Content</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 20px;
        }

        .flashcard-container {
            max-width: 500px;
            width: 100%;
        }

        .header {
            text-align: center;
            margin-bottom: 30px;
            color: white;
        }

        .header h1 {
            font-size: 2.5rem;
            font-weight: 700;
            margin-bottom: 10px;
        }

        .flashcard {
            background: white;
            border-radius: 20px;
            box-shadow: 0 15px 35px rgba(0, 0, 0, 0.3);
            overflow: hidden;
            margin-bottom: 25px;
            transition: transform 0.3s ease;
        }

        .flashcard:hover {
            transform: translateY(-2px);
        }

        .card-header {
            padding: 25px 25px 20px;
            border-bottom: 1px solid #f0f0f0;
        }

        .card-question {
            font-size: 1.4rem;
            font-weight: 700;
            color: #333;
            line-height: 1.4;
        }

        .card-image-container {
            margin: 0 -25px;
            overflow: hidden;
        }

        .card-image-container img {
            width: 100%;
            height: auto;
            max-height: 300px;
            object-fit: cover;
            display: block;
        }

        .card-body {
            padding: 0 25px 25px;
        }

        .answer-section {
            background: #f8f9fa;
            border-radius: 15px;
            padding: 20px;
            margin: 20px 0;
            border-left: 4px solid #667eea;
        }

        .answer-label {
            color: #667eea;
            font-weight: 700;
            font-size: 0.9rem;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-bottom: 12px;
        }

        .card-answer {
            font-size: 1.05rem;
            color: #444;
            line-height: 1.7;
        }

        .card-hint {
            background: linear-gradient(45deg, #e3f2fd, #f3e5f5);
            border-radius: 12px;
            padding: 15px 20px;
            margin-top: 20px;
            font-style: italic;
            color: #1976d2;
            border-left: 4px solid #2196f3;
        }

        .controls {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-top: 30px;
            gap: 15px;
        }

        .btn {
            padding: 12px 24px;
            border: none;
            border-radius: 25px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            font-size: 1rem;
        }

        .btn-primary {
            background: linear-gradient(45deg, #4CAF50, #45a049);
            color: white;
        }

        .btn-secondary {
            background: white;
            color: #333;
            border: 2px solid #ddd;
        }

        .btn:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.2);
        }

        .btn:active {
            transform: translateY(-1px);
        }

        .btn:disabled {
            opacity: 0.4;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }

        .btn:disabled:hover {
            transform: none;
            box-shadow: none;
        }

        .navigation {
            display: flex;
            gap: 10px;
        }

        .study-tip {
            text-align: center;
            color: rgba(255, 255, 255, 0.8);
            font-size: 0.9rem;
            margin-top: 15px;
            font-style: italic;
        }

        /* Mobile Responsiveness */
        @media (max-width: 480px) {
            .flashcard-container {
                max-width: 100%;
            }

            .header h1 {
                font-size: 2rem;
            }
            
            .card-header {
                padding: 20px 20px 15px;
            }

            .card-body {
                padding: 0 20px 20px;
            }

            .card-question {
                font-size: 1.2rem;
            }
            
            .card-answer {
                font-size: 1rem;
            }
            
            .controls {
                flex-wrap: wrap;
                gap: 10px;
            }

            .btn {
                padding: 10px 20px;
                font-size: 0.9rem;
            }

            .card-image-container {
                margin: 0 -20px;
            }

            .card-image-container img {
                max-height: 200px;
            }
        }

        /* Smooth animations */
        .flashcard {
            animation: slideIn 0.5s ease-out;
        }

        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }
    </style>
</head>
<body>
    <div class="flashcard-container">
        <div class="header">
            <h1>Learning Content</h1>
        </div>

        <div class="flashcard" id="flashcard">
            <div class="card-header">
                <div class="card-question" id="question">What is the difference between supervised and unsupervised learning?</div>
            </div>

            <div class="card-image-container" id="imageContainer" style="display: none;">
                <img id="cardImage" alt="Learning content" />
            </div>
            
            <div class="card-body">
                <div class="answer-section">
                    <div class="answer-label">üí° Answer</div>
                    <div class="card-answer" id="answer">
                        <strong>Supervised Learning:</strong> Uses labeled training data to learn patterns and make predictions (e.g., classification, regression).<br><br>
                        <strong>Unsupervised Learning:</strong> Finds hidden patterns in unlabeled data without target outputs (e.g., clustering, dimensionality reduction).
                    </div>
                </div>
                
                <div class="card-hint" id="hintSection">
                    üí° Think: Supervised = teacher guides learning, Unsupervised = self-discovery
                </div>
            </div>
        </div>

        <div class="study-tip">Use arrow keys to navigate</div>

        <div class="controls">
            <button class="btn btn-secondary" id="prevBtn">‚Üê Previous</button>
            <button class="btn btn-primary" id="nextBtn">Next ‚Üí</button>
        </div>
    </div>

    <!-- CARDS DATA - ADD YOUR CARDS HERE -->
    <script>
        const flashcards = [
    {
        question: "What is deep learning and what characterizes it?",
        answer: "Deep learning is an approach to machine learning characterized by <strong>deep stacks of computations</strong>. This depth enables models to disentangle complex and hierarchical patterns in challenging real-world data, leading to human-level or superhuman performance in tasks like <strong>natural language translation</strong>, <strong>image recognition</strong>, and <strong>game playing</strong>.",
        hint: "üß† Think: Deep = many layers of computation stacked together",
        image: ""
    },
    {
        question: "What is a linear unit (neuron) and what does it compute?",
        answer: "A linear unit is the <strong>building block of neural networks</strong>. It computes a linear function:<br><br><strong>Single input:</strong> y = wx + b<br><strong>Multiple inputs:</strong> y = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b<br><br>Where <strong>w</strong> = weight, <strong>x</strong> = input, <strong>b</strong> = bias",
        hint: "‚ö° Remember: Weight √ó Input + Bias = Output",
        image: ""
    },
    {
        question: "How do you create a simple linear model in Keras?",
        answer: "<strong>Basic Keras model syntax:</strong><br><br><code>from tensorflow import keras<br>from tensorflow.keras import layers<br><br>model = keras.Sequential([<br>&nbsp;&nbsp;&nbsp;&nbsp;layers.Dense(units=1, input_shape=[3])<br>])</code><br><br>This creates a model with <strong>1 output unit</strong> accepting <strong>3 input features</strong>.",
        hint: "üíª Sequential = simple stack of layers",
        image: ""
    },
    {
        question: "What does input_shape=[3] mean in Keras?",
        answer: "It tells Keras that the model will accept <strong>3 features as input</strong>. For tabular data, this represents the <strong>number of columns/features</strong> in your dataset.<br><br><strong>Example:</strong> If your data has columns [age, income, score], then input_shape=[3]",
        hint: "üìä Input shape = number of columns in your data",
        image: ""
    },
    {
        question: "What are hidden layers and why are they important?",
        answer: "Hidden layers are layers <strong>between input and output layers</strong>. They enable neural networks to learn <strong>complex, non-linear relationships</strong> by stacking multiple linear units with activation functions.<br><br><strong>Without hidden layers:</strong> Only linear relationships<br><strong>With hidden layers:</strong> Complex patterns and non-linear relationships",
        hint: "üîó Hidden layers = the magic for learning complex patterns",
        image: ""
    },
    {
        question: "What is the ReLU activation function and why is it popular?",
        answer: "<strong>ReLU (Rectified Linear Unit):</strong> max(0, x)<br><br><strong>Function:</strong> Outputs input if positive, otherwise 0<br><br><strong>Why popular:</strong><br>‚Ä¢ Computationally efficient<br>‚Ä¢ Helps with vanishing gradient problem<br>‚Ä¢ Enables learning non-linear patterns<br>‚Ä¢ Simple but effective",
        hint: "üöÄ ReLU: Positive values pass through, negative becomes zero",
        image: ""
    },
    {
        question: "Name three alternatives to ReLU and their characteristics",
        answer: "<strong>ReLU alternatives:</strong><br><br>‚Ä¢ <strong>ELU (Exponential Linear Unit):</strong> Smooth for negative values, can output negative values<br><br>‚Ä¢ <strong>SELU (Scaled ELU):</strong> Self-normalizing properties, good for deep networks<br><br>‚Ä¢ <strong>Swish:</strong> Smooth, non-monotonic function that can outperform ReLU",
        hint: "üîÑ Each activation has unique strengths for different scenarios",
        image: ""
    },
    {
        question: "How do you create a deep neural network in Keras?",
        answer: "<strong>Deep network example:</strong><br><br><code>model = keras.Sequential([<br>&nbsp;&nbsp;&nbsp;&nbsp;layers.Dense(units=4, activation='relu', input_shape=[2]),<br>&nbsp;&nbsp;&nbsp;&nbsp;layers.Dense(units=3, activation='relu'),<br>&nbsp;&nbsp;&nbsp;&nbsp;layers.Dense(units=1)<br>])</code><br><br>Creates a network with <strong>two hidden layers</strong> (4 and 3 units) and <strong>one output layer</strong>",
        hint: "üèóÔ∏è Stack layers: Input ‚Üí Hidden ‚Üí Hidden ‚Üí Output",
        image: ""
    },
    {
        question: "What is Stochastic Gradient Descent (SGD)?",
        answer: "<strong>SGD</strong> is an optimization algorithm that updates model weights using <strong>small random samples (minibatches)</strong> instead of the entire dataset.<br><br><strong>Process:</strong><br>‚Ä¢ Take random sample of training data<br>‚Ä¢ Calculate gradients<br>‚Ä¢ Update weights to minimize loss<br>‚Ä¢ Repeat<br><br><strong>Benefits:</strong> Faster training, often more robust",
        hint: "üéØ SGD: Small steps with small batches = efficient learning",
        image: ""
    },
    {
        question: "Define minibatch and epoch in neural network training",
        answer: "<strong>Key training terminology:</strong><br><br>‚Ä¢ <strong>Minibatch (or batch):</strong> A small random sample of training data used in one SGD iteration<br><br>‚Ä¢ <strong>Epoch:</strong> One complete pass through the entire training dataset<br><br><strong>Example:</strong> 1000 samples with batch size 100 = 10 batches per epoch",
        hint: "üîÑ Batch = small sample, Epoch = complete dataset pass",
        image: ""
    },
    {
        question: "Compare three common loss functions for regression",
        answer: "<strong>Regression loss functions:</strong><br><br>‚Ä¢ <strong>MAE (Mean Absolute Error):</strong> Less sensitive to outliers, measures average absolute differences<br><br>‚Ä¢ <strong>MSE (Mean Squared Error):</strong> Penalizes larger errors more heavily, smooth gradient<br><br>‚Ä¢ <strong>Huber Loss:</strong> Combines MAE and MSE benefits, robust to outliers while maintaining smoothness",
        hint: "üìè Choose based on how you want to handle outliers",
        image: ""
    },
    {
        question: "Compare SGD and Adam optimizers",
        answer: "<strong>SGD vs Adam:</strong><br><br><strong>SGD:</strong><br>‚Ä¢ Simple algorithm<br>‚Ä¢ Uses fixed learning rate<br>‚Ä¢ Can be slow but reliable<br><br><strong>Adam:</strong><br>‚Ä¢ Adaptive learning rates<br>‚Ä¢ Faster convergence<br>‚Ä¢ Combines momentum with adaptive learning<br>‚Ä¢ Generally preferred for most applications",
        hint: "üèÉ‚Äç‚ôÇÔ∏è Adam = smarter, adaptive SGD for faster training",
        image: ""
    },
    {
        question: "How do you compile a Keras model for training?",
        answer: "<strong>Model compilation syntax:</strong><br><br><code>model.compile(<br>&nbsp;&nbsp;&nbsp;&nbsp;optimizer='adam',<br>&nbsp;&nbsp;&nbsp;&nbsp;loss='mae',<br>&nbsp;&nbsp;&nbsp;&nbsp;metrics=['mae']<br>)</code><br><br><strong>Required components:</strong><br>‚Ä¢ <strong>Optimizer:</strong> How to update weights<br>‚Ä¢ <strong>Loss function:</strong> What to minimize<br>‚Ä¢ <strong>Metrics:</strong> What to track during training",
        hint: "‚öôÔ∏è Compile = configure the training process",
        image: ""
    },
    {
        question: "What is overfitting and what are its signs?",
        answer: "<strong>Overfitting:</strong> Model learns training data too well, including noise and irrelevant patterns<br><br><strong>Signs of overfitting:</strong><br>‚Ä¢ High training accuracy, poor validation/test performance<br>‚Ä¢ Training loss decreases while validation loss increases<br>‚Ä¢ Large gap between training and validation metrics",
        hint: "üìàüìâ Good on training, bad on new data = overfitting",
        image: ""
    },
    {
        question: "What is underfitting and how can you identify it?",
        answer: "<strong>Underfitting:</strong> Model is too simple to capture underlying data patterns<br><br><strong>Signs of underfitting:</strong><br>‚Ä¢ Poor performance on both training and validation sets<br>‚Ä¢ Loss plateaus at high value for both training and validation<br>‚Ä¢ Model seems too simple for task complexity",
        hint: "üò¥ Poor performance everywhere = underfitting (too simple)",
        image: ""
    },
    {
        question: "What is model capacity and how does it relate to overfitting/underfitting?",
        answer: "<strong>Model capacity:</strong> The complexity and size of a model (number of parameters)<br><br><strong>Relationship:</strong><br>‚Ä¢ <strong>Low capacity:</strong> Leads to underfitting<br>‚Ä¢ <strong>High capacity:</strong> Can lead to overfitting<br>‚Ä¢ <strong>Goal:</strong> Find optimal capacity for best generalization",
        hint: "‚öñÔ∏è Goldilocks principle: not too simple, not too complex",
        image: ""
    },
    {
        question: "What is early stopping and how does it prevent overfitting?",
        answer: "<strong>Early stopping:</strong> Monitors validation performance and stops training when validation loss starts increasing while training loss continues decreasing<br><br><strong>Prevention mechanism:</strong><br>‚Ä¢ Stops at point of best generalization<br>‚Ä¢ Prevents memorization of training data<br>‚Ä¢ Automatic regularization technique",
        hint: "üõë Stop training at the sweet spot before overfitting begins",
        image: ""
    },
    {
        question: "How can you increase model capacity in neural networks?",
        answer: "<strong>Ways to increase capacity:</strong><br><br>‚Ä¢ <strong>Add more hidden layers</strong> (deeper network)<br>‚Ä¢ <strong>Add more units to existing layers</strong> (wider network)<br>‚Ä¢ <strong>Combine both approaches</strong><br><br>Both methods add more parameters for learning complex patterns",
        hint: "üìèüìê Wider OR deeper OR both = more capacity",
        image: ""
    },
    {
        question: "What is dropout and how does it prevent overfitting?",
        answer: "<strong>Dropout:</strong> Randomly sets a fraction of input units to 0 during training<br><br><strong>Prevention mechanism:</strong><br>‚Ä¢ Prevents over-dependence on specific neurons<br>‚Ä¢ Forces learning of robust, generalizable features<br>‚Ä¢ Creates ensemble effect<br>‚Ä¢ Only applied during training, not inference",
        hint: "üé≤ Randomly 'turn off' neurons to prevent over-reliance",
        image: ""
    },
    {
        question: "How do you implement dropout in Keras?",
        answer: "<strong>Dropout implementation:</strong><br><br><code>model = keras.Sequential([<br>&nbsp;&nbsp;&nbsp;&nbsp;layers.Dense(512, activation='relu'),<br>&nbsp;&nbsp;&nbsp;&nbsp;layers.Dropout(0.3),  # 30% dropout rate<br>&nbsp;&nbsp;&nbsp;&nbsp;layers.Dense(1)<br>])</code><br><br>Dropout layer is typically placed <strong>after Dense layers</strong>",
        hint: "üíß Add Dropout layer after Dense with rate (0.3 = 30%)",
        image: ""
    },
    {
        question: "What is batch normalization and what problems does it solve?",
        answer: "<strong>Batch normalization:</strong> Normalizes inputs to each layer, making training more stable and faster<br><br><strong>Problems it solves:</strong><br>‚Ä¢ Internal covariate shift<br>‚Ä¢ Vanishing/exploding gradients<br>‚Ä¢ Allows higher learning rates<br>‚Ä¢ Acts as mild regularizer",
        hint: "üîÑ Normalize layer inputs = stable, faster training",
        image: ""
    },
    {
        question: "How do you implement batch normalization in Keras?",
        answer: "<strong>Batch normalization implementation:</strong><br><br><code>model = keras.Sequential([<br>&nbsp;&nbsp;&nbsp;&nbsp;layers.Dense(512),<br>&nbsp;&nbsp;&nbsp;&nbsp;layers.BatchNormalization(),<br>&nbsp;&nbsp;&nbsp;&nbsp;layers.Activation('relu'),<br>&nbsp;&nbsp;&nbsp;&nbsp;layers.Dense(1)<br>])</code><br><br>BatchNormalization typically applied <strong>before activation function</strong>",
        hint: "üìä Order: Dense ‚Üí BatchNorm ‚Üí Activation",
        image: ""
    },
    {
        question: "What is the proper order for Dense, BatchNormalization, Dropout, and Activation layers?",
        answer: "<strong>Recommended order:</strong> Dense ‚Üí Batch Normalization ‚Üí Activation ‚Üí Dropout<br><br><code>layers.Dense(units),<br>layers.BatchNormalization(),<br>layers.Activation('relu'),<br>layers.Dropout(rate)</code><br><br>This order optimizes training stability and regularization effectiveness",
        hint: "üî¢ Remember sequence: Dense ‚Üí BatchNorm ‚Üí Activation ‚Üí Dropout",
        image: ""
    },
    {
        question: "How does binary classification differ from regression in neural networks?",
        answer: "<strong>Key differences:</strong><br><br><strong>Binary Classification:</strong><br>‚Ä¢ Predicts one of two classes (0 or 1)<br>‚Ä¢ Uses sigmoid activation in output layer<br>‚Ä¢ Uses binary crossentropy loss<br><br><strong>Regression:</strong><br>‚Ä¢ Predicts continuous values<br>‚Ä¢ No activation (linear) in output layer<br>‚Ä¢ Uses MSE/MAE loss",
        hint: "üéØ Classification = categories, Regression = continuous numbers",
        image: ""
    },
    {
        question: "What is the sigmoid activation function and why use it for binary classification?",
        answer: "<strong>Sigmoid function:</strong> œÉ(x) = 1/(1 + e^(-x))<br><br><strong>Properties:</strong><br>‚Ä¢ Maps any real number to value between 0 and 1<br>‚Ä¢ Perfect for representing probabilities<br>‚Ä¢ Output interpreted as probability of positive class<br>‚Ä¢ Smooth, differentiable function",
        hint: "üìà Sigmoid squashes any number into 0-1 probability range",
        image: ""
    },
    {
        question: "What is binary crossentropy loss?",
        answer: "<strong>Binary crossentropy:</strong> Standard loss function for binary classification<br><br><strong>Formula:</strong> -[y*log(p) + (1-y)*log(1-p)]<br>Where y = true label, p = predicted probability<br><br><strong>Purpose:</strong> Measures difference between predicted probabilities and true binary labels",
        hint: "üìè Measures how far predicted probabilities are from true labels",
        image: ""
    },
    {
        question: "How do you create a binary classification model in Keras?",
        answer: "<strong>Binary classification model:</strong><br><br><code>model = keras.Sequential([<br>&nbsp;&nbsp;&nbsp;&nbsp;layers.Dense(4, activation='relu', input_shape=[33]),<br>&nbsp;&nbsp;&nbsp;&nbsp;layers.Dense(4, activation='relu'),<br>&nbsp;&nbsp;&nbsp;&nbsp;layers.Dense(1, activation='sigmoid')  # sigmoid for binary<br>])<br><br>model.compile(<br>&nbsp;&nbsp;&nbsp;&nbsp;optimizer='adam',<br>&nbsp;&nbsp;&nbsp;&nbsp;loss='binary_crossentropy',<br>&nbsp;&nbsp;&nbsp;&nbsp;metrics=['binary_accuracy']<br>)</code>",
        hint: "üîß Key: sigmoid output + binary_crossentropy loss",
        image: ""
    },
    {
        question: "What is keras.Sequential and when should you use it?",
        answer: "<strong>keras.Sequential:</strong> Easiest way to create neural networks in Keras<br><br><strong>Characteristics:</strong><br>‚Ä¢ Creates linear stack of layers<br>‚Ä¢ Each layer has one input and one output<br>‚Ä¢ Data flows sequentially through layers<br><br><strong>Use when:</strong> Simple, straightforward architectures with linear flow",
        hint: "üìö Sequential = simple stack, one layer after another",
        image: ""
    },
    {
        question: "What does layers.Dense represent and what are its key parameters?",
        answer: "<strong>layers.Dense:</strong> Fully-connected layer where each input connects to each output<br><br><strong>Key parameters:</strong><br>‚Ä¢ <strong>units:</strong> number of neurons/outputs<br>‚Ä¢ <strong>activation:</strong> activation function to apply<br>‚Ä¢ <strong>input_shape:</strong> shape of input data (only needed for first layer)",
        hint: "üï∏Ô∏è Dense = every input connected to every output",
        image: ""
    },
    {
        question: "How do you train a Keras model?",
        answer: "<strong>Model training with fit():</strong><br><br><code>history = model.fit(<br>&nbsp;&nbsp;&nbsp;&nbsp;X_train, y_train,<br>&nbsp;&nbsp;&nbsp;&nbsp;validation_data=(X_valid, y_valid),<br>&nbsp;&nbsp;&nbsp;&nbsp;batch_size=256,<br>&nbsp;&nbsp;&nbsp;&nbsp;epochs=50,<br>&nbsp;&nbsp;&nbsp;&nbsp;verbose=0  # suppress output<br>)</code><br><br><code>fit()</code> trains the model and returns training history",
        hint: "üèÉ‚Äç‚ôÇÔ∏è fit() does the actual training work",
        image: ""
    },
    {
        question: "How do you evaluate model performance in Keras?",
        answer: "<strong>Model evaluation methods:</strong><br><br><strong>Get loss and metrics:</strong><br><code>loss, accuracy = model.evaluate(X_test, y_test)</code><br><br><strong>Make predictions:</strong><br><code>predictions = model.predict(X_test)</code><br><br>Use <strong>evaluate()</strong> for performance metrics, <strong>predict()</strong> for actual predictions",
        hint: "üìä evaluate() = performance, predict() = actual predictions",
        image: ""
    },
    {
        question: "When should you use different activation functions?",
        answer: "<strong>Activation function guidelines:</strong><br><br>‚Ä¢ <strong>ReLU:</strong> Default for hidden layers, fast and effective<br>‚Ä¢ <strong>Sigmoid:</strong> Output layer for binary classification<br>‚Ä¢ <strong>Softmax:</strong> Output layer for multi-class classification<br>‚Ä¢ <strong>Linear (no activation):</strong> Output layer for regression<br>‚Ä¢ <strong>ELU/SELU:</strong> When needing smooth derivatives or self-normalizing",
        hint: "üéõÔ∏è Choose activation based on layer position and task type",
        image: ""
    },
    {
        question: "What are good starting values for learning rates?",
        answer: "<strong>Learning rate effects:</strong><br><br>‚Ä¢ <strong>Too high:</strong> Training unstable, loss explodes<br>‚Ä¢ <strong>Too low:</strong> Very slow training, may get stuck<br><br><strong>Good starting points:</strong><br>‚Ä¢ <strong>SGD:</strong> 0.01<br>‚Ä¢ <strong>Adam:</strong> 0.001<br><br>Use learning rate scheduling or adaptive optimizers for best results",
        hint: "‚ö° Start with 0.001 for Adam, 0.01 for SGD",
        image: ""
    },
    {
        question: "List five strategies to prevent overfitting in deep learning",
        answer: "<strong>Overfitting prevention strategies:</strong><br><br>1. <strong>Early stopping:</strong> Stop when validation performance stops improving<br>2. <strong>Dropout:</strong> Randomly disable neurons during training<br>3. <strong>Batch normalization:</strong> Normalize layer inputs<br>4. <strong>Reduce model complexity:</strong> Fewer layers/units<br>5. <strong>Data augmentation:</strong> Increase training data variety<br>6. <strong>Regularization:</strong> Add L1/L2 penalty terms",
        hint: "üõ°Ô∏è Multiple defenses work better than single approach",
        image: ""
    },
    {
        question: "What should you check when your neural network performs poorly?",
        answer: "<strong>Debugging checklist:</strong><br><br>1. <strong>Data quality:</strong> Errors, missing values, proper preprocessing<br>2. <strong>Model capacity:</strong> Too simple (underfitting) or complex (overfitting)<br>3. <strong>Learning rate:</strong> Too high or too low<br>4. <strong>Loss function:</strong> Appropriate for your task<br>5. <strong>Training time:</strong> Enough epochs vs early stopping<br>6. <strong>Feature scaling:</strong> Normalize/standardize inputs",
        hint: "üîç Systematic debugging: data ‚Üí model ‚Üí training ‚Üí evaluation",
        image: ""
    }
];

        let currentCard = 0;

        // DOM elements
        const flashcard = document.getElementById('flashcard');
        const question = document.getElementById('question');
        const answer = document.getElementById('answer');
        const imageContainer = document.getElementById('imageContainer');
        const cardImage = document.getElementById('cardImage');
        const hintSection = document.getElementById('hintSection');
        const prevBtn = document.getElementById('prevBtn');
        const nextBtn = document.getElementById('nextBtn');

        function updateButtonStates() {
            // Disable Previous button on first card
            if (currentCard === 0) {
                prevBtn.disabled = true;
            } else {
                prevBtn.disabled = false;
            }
            
            // Disable Next button on last card
            if (currentCard === flashcards.length - 1) {
                nextBtn.disabled = true;
            } else {
                nextBtn.disabled = false;
            }
        }

        function updateCard() {
            const card = flashcards[currentCard];
            
            // Add slide-in animation
            flashcard.style.animation = 'none';
            flashcard.offsetHeight; // Trigger reflow
            flashcard.style.animation = 'slideIn 0.5s ease-out';
            
            // Update content
            question.textContent = card.question;
            answer.innerHTML = card.answer;
            
            // Update hint
            if (card.hint) {
                hintSection.textContent = card.hint;
                hintSection.style.display = 'block';
            } else {
                hintSection.style.display = 'none';
            }
            
            // Update button states
            updateButtonStates();
        }

        function nextCard() {
            currentCard = (currentCard + 1) % flashcards.length;
            updateCard();
        }

        function prevCard() {
            currentCard = (currentCard - 1 + flashcards.length) % flashcards.length;
            updateCard();
        }

        function shuffleCards() {
            for (let i = flashcards.length - 1; i > 0; i--) {
                const j = Math.floor(Math.random() * (i + 1));
                [flashcards[i], flashcards[j]] = [flashcards[j], flashcards[i]];
            }
            currentCard = 0;
            updateCard();
        }

        // Event listeners
        nextBtn.addEventListener('click', nextCard);
        prevBtn.addEventListener('click', prevCard);

        // Keyboard navigation
        document.addEventListener('keydown', (e) => {
            switch(e.key) {
                case 'ArrowLeft':
                    prevCard();
                    break;
                case 'ArrowRight':
                    nextCard();
                    break;
                case 'Enter':
                    e.preventDefault();
                    nextCard();
                    break;
            }
        });

        // Initialize
        updateCard();
    </script>
</body>
</html>
